# FineTunedSBERT
Text Similarity Detection and Sentiment Analysis of a Large Text via Fine-Tuned SBERT

## Abstract
I present a fine-tined Sentence Transformer (SBERT) architecture for encoding textual prompts, primarily focusing on discerning the similarity between two text prompts while considering their contextual nuances. The dataset employed for this text similarity detection task comprised pairs of questions from Quora. The approach involved initially generating raw embeddings through SBERT for each prompt pair, followed by computing cosine similarity between them. Pairs exhibiting a similarity greater than the 0.5 threshold were classified as similar. To enhance accuracy, I trained a Logistic Regression classifier using the generated embeddings and subsequently evaluated the model on a validation set. Post-evaluation, the model became adept at discerning the similarity between testing arbitrary statement pairs.

Furthermore, I fine-tuned the SBERT architecture, tailored specifically to the dataset at hand, for text sentiment analysis. For this particular task, I curated a dataset consisting of both negative and positive reviews of TV shows. Notably, I adapted the architecture of distiluse-base-multilingual-cased-v1, incorporating a dropout layer preceding the fully-connected dense layer. The model underwent training for 600 epochs, with a learning rate of 0.002, and was evaluated based on cross-entropy loss.

## Methodology
### Text Similarity
I used Sentence-BERT (SBERT) model – a modification of the pretrained BERT network that uses siamese and triplet network structures to derive semantically meaningful sentence embeddings that can be compared using cosine-similarity – to calculate our text embeddings. Our pipeline for pre-trained model was as follows:

![Approach_screenshot](https://github.com/MohtashimButt/FineTunedSBERT/blob/master/approach.png)

Now that I've used a pre-trained model and saw how it performed (without any training, since it was pre-trained), I wanted to test it on a classifer trained on the Quora Question Pair data. I trained a Logistic Regression model (using L2 loss and over 10000 epochs) with the embeddings generated by the SBERT and tested it (with the val split we made earlier). 

![Approach_screenshot](https://github.com/MohtashimButt/FineTunedSBERT/blob/master/approach2.png)

The calculated confusion matrix after comparing the results from the predicted labels with the actual labels given in the data is as follows:

![Approach_screenshot](https://github.com/MohtashimButt/FineTunedSBERT/blob/master/conf_mat.png)

The evaluation metrics are as follows:

| Metric | Value |
| -------- | -------- |
| Accuracy | 0.7397111111111111 |
| Precision | 0.635419469407523 |
| Recall | 0.7051236116087424 |
| F1 Score | 0.6684593393529394 |

### Sentiment Analysis
I used `distiluse-base-multilingual-cased-v1` which is good for generating embeddings for large texts on low computation power. I added a dropout layer preceding the fully-connected dense layer in the novel architecture. The puspose of adding a dropout layer was to reduce the computational cost and minimize the overfitting of the model. The fully connected dense layer was added to retain the context of each text prompt. As I mentioned earlier, the dataset for sentiment analysis is different from the one used for similarity deteciton. In similarity detection, I used Quora question pair dataset while in sentiment analysis, I used positive and negative reviews of the TV-Shows.

![Approach_screenshot](https://github.com/MohtashimButt/FineTunedSBERT/blob/master/SBERT.png)

The model gives the following outputs on the below mentioned testing statements:
'I love being a human being. We should live in peace.' has positive sentiments
'Wage war and kill people in blood' has negative sentiments

The evaluation report is as follows:

| Metric | Value |
| -------- | -------- |
| Accuracy | 0.62 |
| Precision | 0.63 |
| Recall | 0.61 |
| F1 Score | 0.62 |

## Limitations and Discussion
The key limitations in the project are as follows:
- The dataset used was for movies/TV-Show reviews that represents sentiments including the words specific to the showbiz. If the model is given the prompts that do not have any context in the showbiz world, it won't be able to recognize its sentiments.
- Due to the computational limits I had, the dataset size was small over which a world-class model like BERT cannot be trained enough to give meaningful accuracy. That is why the evaluation metrics are not that good.
- Although the classification is working is pretty well but the primary use-case of state-of-the-art BERT model does lie in sentiment analysis. Hence, the results would've been better if I had used some other language model for embedding generation. 
